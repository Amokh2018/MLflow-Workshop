{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment <br>\n",
    "\n",
    "Python 3.8 <br>\n",
    "Numpy: 1.23.0 <br>\n",
    "Pandas: 1.5.3 <br>\n",
    "matplotlib: 3.7.1 <br>\n",
    "seaborn: 0.10.1 <br>\n",
    "Scikit-Learn: 1.1.3 <br>\n",
    "MLFlow: 1.30.0 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib #\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn #\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, plot_roc_curve, confusion_matrix\n",
    "from sklearn.model_selection import KFold \n",
    "import mlflow\n",
    "import mlflow.sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numpy: {}\".format(np.__version__))\n",
    "print(\"Pandas: {}\".format(pd.__version__))\n",
    "print(\"matplotlib: {}\".format(matplotlib.__version__))\n",
    "print(\"seaborn: {}\".format(sns.__version__))\n",
    "print(\"Scikit-Learn: {}\".format(sklearn.__version__))\n",
    "print(\"MLFlow: {}\".format(mlflow.__version__))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from this link : https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"creditcard.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.drop(\"Time\", axis=1)\n",
    "# remove time column since it was found that it is not helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data to normal and anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = df[df.Class == 0].sample(frac=0.5, random_state=2020).reset_index(drop=True) \n",
    "anomaly = df[df.Class == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Normal: {normal.shape}\")\n",
    "print(f\"Anomaly: {anomaly.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train/ dev/ test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_train, normal_test = train_test_split(normal, test_size = 0.2, random_state = 2020)\n",
    "anomaly_train, anomaly_test = train_test_split(anomaly, test_size = 0.2, random_state = 2020)\n",
    "normal_train, normal_validate = train_test_split(normal_train,test_size = 0.25, random_state = 2020)\n",
    "anomaly_train, anomaly_validate = train_test_split(anomaly_train, test_size = 0.25, random_state = 2020)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate set to creates X-Y sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.concat((normal_train, anomaly_train))\n",
    "x_test = pd.concat((normal_test, anomaly_test))\n",
    "x_validate = pd.concat((normal_validate,anomaly_validate))\n",
    "\n",
    "y_train = np.array(x_train[\"Class\"])\n",
    "y_test = np.array(x_test[\"Class\"])\n",
    "y_validate = np.array(x_validate[\"Class\"])\n",
    "\n",
    "x_train = x_train.drop(\"Class\", axis=1)\n",
    "x_test = x_test.drop(\"Class\", axis=1)\n",
    "x_validate = x_validate.drop(\"Class\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training sets:\\nx_train: {} \\ny_train:{}\".format(x_train.shape, y_train.shape))\n",
    "print(\"\\nTesting sets:\\nx_test: {} \\ny_test:{}\".format(x_test.shape, y_test.shape))\n",
    "print(\"\\nValidation sets:\\nx_validate: {} \\ny_validate: {}\".format(x_validate.shape, y_validate.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat((normal, anomaly)).drop(\"Class\", axis=1))\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_validate = scaler.transform(x_validate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sk_model, x_train, y_train): \n",
    "    sk_model = sk_model.fit(x_train, y_train)\n",
    "    train_acc = sk_model.score(x_train, y_train)\n",
    "    mlflow.log_metric(\"train_acc\", train_acc)\n",
    "    print(f\"Train Accuracy: {train_acc:.3%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sk_model, x_test, y_test):\n",
    "    eval_acc = sk_model.score(x_test, y_test)\n",
    "    preds = sk_model.predict(x_test)\n",
    "    auc_score = roc_auc_score(y_test, preds)\n",
    "    # ask MLFlow to log two more metrics\n",
    "    mlflow.log_metric(\"eval_acc\", eval_acc)\n",
    "    mlflow.log_metric(\"auc_score\", auc_score)\n",
    "    print(f\"Auc Score: {auc_score:.3%}\")\n",
    "    print(f\"Eval Accuracy: {eval_acc:.3%}\")\n",
    "    roc_plot = plot_roc_curve(sk_model, x_test, y_test,name='Scikit-learn ROC Curve')\n",
    "    plt.savefig(\"sklearn_roc_plot.png\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, preds)\n",
    " \n",
    "    ax = sns.heatmap(conf_matrix, annot=True,fmt='g') \n",
    "    ax.invert_xaxis()\n",
    "    ax.invert_yaxis()\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title(\"Confusion Matrix\") \n",
    "    plt.savefig(\"sklearn_conf_matrix.png\")\n",
    "    # save the plots generated by matplotlib and by seaborn.\n",
    "    mlflow.log_artifact(\"sklearn_roc_plot.png\")\n",
    "    mlflow.log_artifact(\"sklearn_conf_matrix.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log and View MLFlow Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_model = LogisticRegression(random_state=None, max_iter=400, solver='newton-cg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puts the run under experiment name\n",
    "mlflow.set_experiment(\"Credit_Card_Fraud_experiment\") \n",
    "mlflow.set_tracking_uri(\"localhost:5000\")\n",
    "# chunk all code under the context of one MLFlow run.\n",
    "with mlflow.start_run():\n",
    "    train(sk_model, x_train, y_train)\n",
    "    evaluate(sk_model, x_test, y_test)\n",
    "    # save the model\n",
    "    mlflow.sklearn.log_model(sk_model, \"log_reg_model\")\n",
    "    print(\"Model run: \", mlflow.active_run().info.run_uuid)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On terminal, write: mlflow ui -p 1234\n",
    "so you can access mlflow UI\n",
    "\n",
    "and then go to 127.0.0.1:1234"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Logged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = mlflow.sklearn.load_model(\"runs:/d96759e62aaf44dba44dc50575de258a/log_reg_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.score(x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLFlow Parameter Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broad Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_weights = [1, 5, 10, 15]\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"sklearn_creditcard_broad_search\") \n",
    "logs = []\n",
    "for f in range(len(anomaly_weights)):\n",
    "    fold = 1\n",
    "    accuracies = []\n",
    "    auc_scores= []\n",
    "    for train, test in kfold.split(x_validate, y_validate):\n",
    "        with mlflow.start_run():\n",
    "            weight = anomaly_weights[f] \n",
    "            mlflow.log_param(\"anomaly_weight\", weight)\n",
    "            class_weights= {0: 1,1: weight }\n",
    "            sk_model = LogisticRegression(random_state=None, max_iter=400,\n",
    "                                        solver='newton-cg',\n",
    "                                        class_weight=class_weights).fit(x_validate[train],y_validate[train])\n",
    "            for h in range(40): print('-', end=\"\") \n",
    "            print(f\"\\nfold {fold}\\nAnomaly Weight: {weight}\")\n",
    "            train_acc = sk_model.score(x_validate[train], y_validate[train])\n",
    "            mlflow.log_metric(\"train_acc\", train_acc)\n",
    "            eval_acc = sk_model.score(x_validate[test], y_validate[test])\n",
    "            preds = sk_model.predict(x_validate[test])\n",
    "            mlflow.log_metric(\"eval_acc\", eval_acc)\n",
    "\n",
    "            try:\n",
    "                auc_score = roc_auc_score(y_validate[test], preds)\n",
    "            except:\n",
    "                auc_score = -1\n",
    "            mlflow.log_metric(\"auc_score\", auc_score)\n",
    "            print(\"AUC: {}\\neval_acc: {}\".format(auc_score,\n",
    "            eval_acc))\n",
    "            accuracies.append(eval_acc)\n",
    "            auc_scores.append(auc_score)\n",
    "            log = [sk_model, x_validate[test],\n",
    "            y_validate[test], preds]\n",
    "            logs.append(log)\n",
    "            mlflow.sklearn.log_model(sk_model,\n",
    "            f\"anom_weight_{weight}_fold_{fold}\")\n",
    "            fold = fold + 1\n",
    "            mlflow.end_run()\n",
    "    print(\"\\nAverages: \")\n",
    "    print(\"Accuracy: \", np.mean(accuracies))\n",
    "    print(\"AUC: \", np.mean(auc_scores))\n",
    "    print(\"Best: \")\n",
    "    print(\"Accuracy: \", np.max(accuracies))\n",
    "    print(\"AUC: \", np.max(auc_scores))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_weights = [10, 50, 150, 200]\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=2020)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new experiment for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"sklearn_creditcard_Guided_search\") \n",
    "logs = []\n",
    "for f in range(len(anomaly_weights)):\n",
    "    fold = 1\n",
    "    accuracies = []\n",
    "    auc_scores= []\n",
    "    for train, test in kfold.split(x_validate, y_validate):\n",
    "        with mlflow.start_run():\n",
    "            weight = anomaly_weights[f] \n",
    "            mlflow.log_param(\"anomaly_weight\", weight)\n",
    "            class_weights= {0: 1,1: weight }\n",
    "            sk_model = LogisticRegression(random_state=None, max_iter=400,\n",
    "                                        solver='newton-cg',\n",
    "                                        class_weight=class_weights).fit(x_validate[train],y_validate[train])\n",
    "            for h in range(40): print('-', end=\"\") \n",
    "            print(f\"\\nfold {fold}\\nAnomaly Weight: {weight}\")\n",
    "            train_acc = sk_model.score(x_validate[train], y_validate[train])\n",
    "            mlflow.log_metric(\"train_acc\", train_acc)\n",
    "            eval_acc = sk_model.score(x_validate[test], y_validate[test])\n",
    "            preds = sk_model.predict(x_validate[test])\n",
    "            mlflow.log_metric(\"eval_acc\", eval_acc)\n",
    "\n",
    "            try:\n",
    "                auc_score = roc_auc_score(y_validate[test], preds)\n",
    "            except:\n",
    "                auc_score = -1\n",
    "            mlflow.log_metric(\"auc_score\", auc_score)\n",
    "            print(\"AUC: {}\\neval_acc: {}\".format(auc_score,\n",
    "            eval_acc))\n",
    "            accuracies.append(eval_acc)\n",
    "            auc_scores.append(auc_score)\n",
    "            log = [sk_model, x_validate[test],\n",
    "            y_validate[test], preds]\n",
    "            logs.append(log)\n",
    "            mlflow.sklearn.log_model(sk_model,\n",
    "            f\"anom_weight_{weight}_fold_{fold}\")\n",
    "            fold = fold + 1\n",
    "            mlflow.end_run()\n",
    "    print(\"\\nAverages: \")\n",
    "    print(\"Accuracy: \", np.mean(accuracies))\n",
    "    print(\"AUC: \", np.mean(auc_scores))\n",
    "    print(\"Best: \")\n",
    "    print(\"Accuracy: \", np.max(accuracies))\n",
    "    print(\"AUC: \", np.max(auc_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6df5fb1b25f0df95286e840a09a7d4433b68f80dbec77cb3e9268a8c02167994"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
